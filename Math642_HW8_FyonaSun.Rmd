---
title: "Math642_HW8_FyonaSun"
author: "Fyona Sun"
date: "3/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 8.1

```{r}
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(0,100), ylim = c(0,100), xlab = "X", ylab = "Y")
lines(x = c(40,40), y = c(0,100))
text(x = 40, y = 108, labels = c("t1"), col = "red")
lines(x = c(0,40), y = c(75,75))
text(x = -8, y = 75, labels = c("t2"), col = "red")
lines(x = c(75,75), y = c(0,100))
text(x = 75, y = 108, labels = c("t3"), col = "red")
lines(x = c(20,20), y = c(0,75))
text(x = 20, y = 80, labels = c("t4"), col = "red")
lines(x = c(75,100), y = c(25,25))
text(x = 70, y = 25, labels = c("t5"), col = "red")

text(x = (40+75)/2, y = 50, labels = c("R1"))
text(x = 20, y = (100+75)/2, labels = c("R2"))
text(x = (75+100)/2, y = (100+25)/2, labels = c("R3"))
text(x = (75+100)/2, y = 25/2, labels = c("R4"))
text(x = 30, y = 75/2, labels = c("R5"))
text(x = 10, y = 75/2, labels = c("R6"))
```
## 8.4
This question relates to the plots in Figure 8.12.
(a) Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of Figure 8.12. The numbers inside the boxes indicate the mean of Y within each region.

```{r}
library(diagram)
par(mar = c(4, 4, 4, 4))
openplotmat()
elpos <- coordinates(c(1, 2, 2, 2,2))
fromto <- matrix(ncol = 2, byrow = TRUE, data = c(1, 2, 1, 3, 
                                                  2, 4, 2, 5,
                                                  4, 6, 4, 7,
                                                  7, 8, 7, 9))
nr <- nrow(fromto)
arrpos <- matrix(ncol = 2, nrow = nr)
for (i in 1:nr) arrpos[i, ] <- straightarrow (to = elpos[fromto[i, 2], ],
                                from = elpos[fromto[i, 1], ],
                                lwd = 1, arr.pos = 0.7, arr.length = 0.15)
 
 
labels <- c("X1<=1","X2<=1","5","X1<=0","15","3","X2<=0","10","0")
 
for (i in 1:length(labels)) {
  textellipse(elpos[i,], 0.1, 0.05, lab = labels[i], box.col = "white", shadow.size = 0.005, cex = 2)
}
 
text(arrpos[1, 1]  - .01 , arrpos[1, 2], "yes")
text(arrpos[2, 1]  + .01, arrpos[2, 2], "no")
text(arrpos[3, 1]  - .01 , arrpos[3, 2], "yes")
text(arrpos[4, 1]  + .01, arrpos[4, 2], "no")
text(arrpos[5, 1]  - .01 , arrpos[5, 2], "yes")
text(arrpos[6, 1]  + .01, arrpos[6, 2], "no")
text(arrpos[7, 1]  - .01, arrpos[7, 2], "yes")
text(arrpos[8, 1]  + .01, arrpos[8, 2], "no")
```

(b) Create a diagram similar to the left-hand panel of Figure 8.12, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region.
```{r}
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")
lines(x = c(-2, 2), y = c(1, 1))
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8),col = "red")
text(x = 1.5, y = -1, labels = c(0.63),col = "red")
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49),col = "red")
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06),col = "red")
text(x = 1, y = 1.5, labels = c(0.21),col = "red")
```
## 8.9
9. This problem involves the OJ data set which is part of the ISLR package.

(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
```{r}
library(ISLR)
library(caret)
attach(OJ)
set.seed(1)
Train <- createDataPartition(OJ$Purchase, p = 800/1070, list = FALSE)
training <- OJ[Train,]
testing <- OJ[-Train,]
```
(b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
```{r}
library(rpart)
rpart_model <- rpart(Purchase ~ ., data = training, method = 'class',
                     control = rpart.control(cp = 0))
summary(rpart_model, cp = 1)

postResample(predict(rpart_model, training, type = 'class'), training$Purchase)
```

The model summary shows that the variable LoyalCH is the most important for determining which orange juice a customer will buy. The tree model has an accuracy of 86.76654% and 1 node. 

(c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
```{r}
rpart_model
```
The root is split into nodes using the varable LoyalCH. If a customer scored LoyalCH is larger than 0.48 they are predicted to be in the class CH, which means we expect them to buy CH over MM. In CH class if the LoyalCH score is less than 0.76 then we will continue to examine whether the ListPriceDiff is greater than 0.24. If the ListPriceDiff is less than 0.24, the observation would be in the class MM. Then we check the number of stores. If the number of stores is greater than 4 then the observation is in class MM, otherwise it's in class CH.

We can see from the output that the accuracy of that node is 81.9%

(d) Create a plot of the tree, and interpret the results.
```{r}
library(rpart.plot)
rpart.plot(rpart_model)
```
(e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
```{r}
pred <- predict(rpart_model, testing, type = 'class')

caret::confusionMatrix(pred, testing$Purchase)
```
The test accuracy is 81.78%

(f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.
```{r}
rpart_cv_model <- train(training[,-1], training[,1],
                        method = 'rpart',
                        trControl = trainControl(method = 'cv', number = 10),
                        tuneGrid = expand.grid(cp = seq(0, 0.5, length.out = 10)))

rpart_cv_model
```
(g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
```{r}
plot(rpart_cv_model)
```
(h) Which tree size corresponds to the lowest cross-validated classi- fication error rate?
```{r}
rpart_cv_model$bestTune

rpart_cv_model$results
```
(i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
```{r}
set.seed(1)
rpart_tuned <- rpart(Purchase ~ ., data = training, method = 'class',
                     control = rpart.control(cp = 0.02))
rpart_tuned
rpart.plot(rpart_tuned)
```
(j) Compare the training error rates between the pruned and un- pruned trees. Which is higher?
```{r}
postResample(predict(rpart_model, 
                     training,
                     type = 'class'), training$Purchase)

postResample(predict(rpart_tuned, 
                     training,
                     type = 'class'), training$Purchase)
```

The unpruned model has higher training accuracy. This does not mean we should never prune. It just means that the training set is well representative of the testing set.

(k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r}
postResample(predict(rpart_model, 
                     testing,
                     type = 'class'), testing$Purchase)

postResample(predict(rpart_tuned, 
                     testing,
                     type = 'class'), testing$Purchase)
```

The unpruned model has higher accuracy.

## 8.10
10. We now use boosting to predict Salary in the Hitters data set.

(a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries.

```{r}
library(data.table)
Hitters<- read.csv('~/Math642_FyonaSun/Hitters.csv')
setDT(Hitters)
Hitters <-  Hitters[-which(is.na(Hitters$Salary)), ]
sum(is.na(Hitters$Salary))
Hitters$Salary <-  log(Hitters$Salary)
```
(b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.
```{r}
train <-  1:200
train.hitters <-  Hitters[train, ]
test.hitters <-  Hitters[-train, ]
```
(c) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter lambda. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.
```{r}
library(gbm)
set.seed(1)

pows <-  seq(-10, -0.2, by=0.1)
lambdas <-  10 ^ pows
train.errors <-  rep(NA, length(lambdas))
test.errors <-  rep(NA, length(lambdas))

for (i in 1:length(lambdas)) {
  boost.hitters <-  gbm(Salary ~ AtBat+Hits+HmRun+Runs+RBI+Walks+Years+CAtBat+CHits+CHmRun +CRuns+CRBI+CWalks+League+Division+PutOuts+Assists+Errors+NewLeague, data=train.hitters,
                        distribution="gaussian",
                        n.trees=1000,
                        shrinkage=lambdas[i])
  train.pred <-  predict(boost.hitters, train.hitters, n.trees=1000)
  test.pred <-  predict(boost.hitters, test.hitters, n.trees=1000)
  train.errors[i] <-  mean((train.hitters$Salary - train.pred)^2)
  test.errors[i] <-  mean((test.hitters$Salary - test.pred)^2)
}

plot(lambdas, train.errors, type="b", 
     xlab="Shrinkage", ylab="Train MSE", 
     col="blue", pch=20, bty = "n")
```
(d) Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.
```{r}
plot(lambdas, test.errors, type="b", 
     xlab="Shrinkage", ylab="Test MSE", 
     col="orange", pch=20)

min(test.errors)
lambdas[which.min(test.errors)]
```
The smallest test MSE of boosting is 0.2540265.

(e) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.
```{r}
library(glmnet)
fitlm = lm(Salary ~ AtBat+Hits+HmRun+Runs+RBI+Walks+Years+CAtBat+CHits+CHmRun +CRuns+CRBI+CWalks+League+Division+PutOuts+Assists+Errors+NewLeague, data = train.hitters)
pred = predict(fitlm, test.hitters)
mean((pred - test.hitters$Salary)^2)
```

```{r}
set.seed(1)

x <-  model.matrix(Salary ~ . , data=train.hitters)
y <-  train.hitters$Salary
x.test <-  model.matrix(Salary ~ . , data=test.hitters)
lasso.fit <-  glmnet(x, y, alpha=1)
lasso.pred <-  predict(lasso.fit, s=0.01, newx=x.test)
mean((test.hitters$Salary - lasso.pred)^2)
```

The linear model and the Lasso have higher test MSE than boosting.

(f) Which variables appear to be the most important predictors in the boosted model?
```{r}
boost.best <-  gbm(Salary ~ AtBat+Hits+HmRun+Runs+RBI+Walks+Years+CAtBat+CHits+CHmRun +CRuns+CRBI+CWalks+League+Division+PutOuts+Assists+Errors+NewLeague, data=train.hitters,
                   distribution="gaussian", n.trees=1000,
                   shrinkage=lambdas[which.min(test.errors)])
summary(boost.best)
```
CAtBat, CRBI and CWalks are the most important variables.

(g) Now apply bagging to the training set. What is the test set MSE for this approach?
```{r}
library(randomForest)
set.seed(1)
fit.rf <-  randomForest(Salary ~ AtBat+Hits+HmRun+Runs+RBI+Walks+Years+CAtBat+CHits+CHmRun +CRuns+CRBI+CWalks+League+Division+PutOuts+Assists+Errors+NewLeague, data=train.hitters, 
                            ntree=500, mtry=19)
pred.rf <-  predict(fit.rf, test.hitters)
mean((test.hitters$Salary - pred.rf)^2)
```

Test MSE for random forest bagging is about 0.2299324, which is slightly better than the smallest test MSE for boosting.